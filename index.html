<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$']]
              }
            };
          </script>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Effective Reinforcement Learning for Reasoning in Language Models</title>


            <style>
                /* Styling for the table */
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                }

                .table-container {
                    margin-right: 10px;
                }


                /* Styling for the carousel */
                .carousel-container {
                    width: 100%;
                    max-width: 600px;
                    margin: auto;
                    position: relative;
                    overflow: hidden;
                    align-items: center;
                    padding: 0 50px; /* Space for arrows */
                }

                .carousel-slide {
                    display: none;
                    max-width: 100%; /* Adjusted to use the full area within the padding */
                    height: auto;
                    position: relative;
                    margin: auto;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                }

                .carousel-slide.active {
                    display: flex;
                }

                .carousel-slide img {
                    max-width: 100%;
                    max-height: 100%;
                    width: auto;
                    height: auto;
                    margin: 0;
                }

                .carousel-controls {
                    position: absolute;
                    top: 10px; /* Adjust this value to set how far from the top the arrows should start */
                    left: 0;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                }

                .carousel-button {
                    position: absolute;
                    top: 75px; /* Keeps arrows at 10px from the top of the container */
                    transform: translateY(-50%); /* Center vertically relative to their height */
                    z-index: 10;
                    background: rgba(255, 255, 255, 0.8); /* Semi-transparent white background */
                    color: #000; /* Black arrow color for visibility */
                    border: none;
                    cursor: pointer;
                    font-size: 24px; /* Size of the arrow icons */
                    padding: 10px; /* Padding around the arrows */
                    height: 50px; /* Adjust this value to change the height */
                    width: 50px; /* Adjust width to maintain proportion if necessary */
                    display: flex;
                    align-items: center;
                    justify-content: center;
                }


                .carousel-button:first-child {
                    left: 0; /* Position the left arrow slightly outside the image area */
                }

                .carousel-button:last-child {
                    right: 0; /* Position the right arrow slightly outside the image area */
                }

                .carousel-caption p {
                    text-align: center;
                    font-size: 16px;
                    color: #333;
                    margin-top: 10px;
                }



                /* Additional styles can go here */
                /* .image-container {
                display: flex;
                gap: 16px;
                }
                .image-container img {
                flex: 1 1 0;
                max-width: 100%;
                height: auto;
                }
                @media (max-width: 600px) {
                .image-container {
                    flex-direction: column;
                }
                } */
            </style>

    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Effective Reinforcement Learning for Reasoning in Language Models</h1>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2505.17218" class="button">Paper</a>
                <a href="https://github.com/shuoli90/efficient_reasoning" class="button">Code</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/header.png" alt="Teaser Image" class="teaser-image">
            </div>
        </div>
    <d-article>

        <!-- Horizontal byline starts here -->
        <div class="byline-horizontal">
            <div class="authors">
                <p>
                    <a href="https://lhleohuang.github.io/" class="author-link">Lianghuan Huang*</a>,
                    <a href="https://shuoli90.github.io/" class="author-link">Shuo Li*</a>,
                    <a href="https://sagnikanupam.com/" class="author-link">Sagnik Anupam</a>,
                    <a href="https://www.cis.upenn.edu/~lee/home" class="author-link">Insup Lee</a>,
                    <a href="https://obastani.github.io/" class="author-link">Osbert Bastani</a>
            </div>
            <div class="affiliations">
                University of Pennsylvania
            </div>
            <div class="contributions">
                *Equal Contribution
            </div>
        </div>

    <script>



    var bibliography = {
        "1": "Shen, Bokui, et al. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes.2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2021."
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    let slideIndex = 0; // Global slide index

    document.addEventListener('DOMContentLoaded', (event) => {
        showSlides(slideIndex); // Initialize slides
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString()); // Insert citations
        }
        updateBibliography(); // Update bibliography
    });

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0; }
        if (n < 0) { slideIndex = slides.length - 1; }

        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block"; // Display only the active slide
    }

</script>


        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#sftvsrl"> SFT vs. On-Policy RL</a></div>
                <div><a href="#dashvsgrpo"> DASH vs. GRPO</a></div>
                <div><a href="#pgvsppo"> Policy Gradient vs. PPO Gradient Updates</a></div>
                <div><a href="#kl"> KL Divergence Regularization</a></div>
                <div><a href="#conclusion"> Conclusion</a></div>

            </nav>
        </d-contents>

        <d-figure>
            <img src="images/side_by_side_aligned.png" width="100%">
            <!-- <div class="image-container">
                <img src="images/time.png" alt="Time Comparison">
                <img src="images/preemptive_sampling.png" alt="Preemptive Sampling">
            </div> -->
            <p>
                <br> 
<strong>Post-training LMs with RL often demand massive compute and time. Since inference is far more memory-efficient than backpropagation, much larger batches are optimal for sampling. Empirically, sampling dominates total training time </strong>(right figure). We introduce the Distributed Aggregated Sampling Handler (DASH) (left figure) that dramatically reduces sampling time and accelerates training. DASH combines preemptive sampling (sampling of large batches with backpropagation in smaller chunks) and gradient filtering (discarding samples with low advantage estimates). It plugs into any sampling-based RL algorithm (on-policy or off-policy) and, versus GRPO, <strong>cuts training time by 83% without loss of accuracy</strong> (right figure). 
<br> <br> Additionally, given that most RL methods were originally built for robotics rather than LM reasoning, we <strong>analyze design decisions</strong> for LM-focused RL to balance accuracy and efficiency on smaller models. We use Qwen2.5-{0.5B, 1.5B, 3B} models as our base models, all of which are not already post-trained.
We use the MATH-500 dataset for training and in-distribution evaluation, and the GSM8K dataset for out-of-distribution evaluation. For coding we use the MBPP+ dataset.

            </p>
        </d-figure>

        <section id="sftvsrl">
            <h2>SFT vs. On-Policy RL</h2>
<h3>For the models we consider, we find on-policy RL to be effective but not SFT </h3>
            <p>
We compare three algorithms: (i) SFT with human-written reasoning traces, denoted SFT-H, (ii) SFT with reasoning traces from Qwen2.5-7B-Instruct, denoted SFT-M, and (iii) DASH. Results are shown in the table below. As can be seen, DASH improves performance both in-distribution and out-of-distribution, demonstrating that on-policy algorithms can efficiently learn mathematical reasoning skills that generalize across datasets. On the other hand, neither SFT-H nor SFT-M improve performance, with SFT-H significantly degrading both in-distribution and out-of-distribution performance. Intuitively, the substantial performance degradation caused by SFT-H can be attributed to the fact that human reasoning often omits many intermediate steps, which is especially problematic for smaller LMs.
<br> <br> 
For coding, we train on human programs in MBPP+. As shown in the table below, DASH outperforms SFT
in most cases, demonstrating the the general effectiveness of on-policy RL at improving the reasoning capabilities of LMs.
To the best of our knowledge, these are among the first results to show that on-policy RL can improve code generation for
smaller LMs.

            </p>
            <d-figure>
                <img src="images/sftvsrl.png" width="100%">
            </d-figure>

            

    </section>


  <section id="dashvsgrpo">
        <h2> DASH vs. GRPO</h2>
        <h3> Preemptive and Parallel Sampling</h3>
We sample a large number of trajectories in one batch, and then perform backpropagation on these samples in smaller batches. <strong>Preemptive sampling can be further sped up by using multiple inference servers in parallel.</strong>  We note that parallel sampling provides the amortized speed-up <strong>only when sampling is done in large batches</strong>. This is due to internal speed-up mechanisms for single sampling servers when the batch size is small. We also note that <strong>to avoid out-of-memory issues</strong> on the backpropagation servers, large batches of samples are <strong>cached and delivered  <i>upon request</i>.</strong>
<br> <br> 
            <d-figure>
                <center>
                <img src="images/preemptive_sampling.png" width="60%">
                <figcaption>
                    <br>
                    Illustration of preemptive sampling. We use $H$ GPUs for inference and $H'$ for backpropagation; they are shown in blue and green, respectively. Given a batch of $M$ prompts $\{x_1, \ldots, x_M\}$. The inference GPUs then generate corresponding responses $\{\hat{y}_1, \ldots, \hat{y}_M\}$, which are aggregated across GPUs into CPU memory. When a backpropagation GPU requests generations for a prompt $x_m$, the corresponding cached response $y_m$ is retrieved and delivered. Since we are using groups for advantage estimation, each prompt $x_m$ is duplicated to form groups, and all generations in the same group are sent to the backpropagation GPU upon request.
                </figcaption>
                </center>
            </d-figure>

<h3>Gradient Filtering</h3>
<p>We drop examples with small advantage estimates. If the advantage estimate is small, then the contribution to the gradient is likely to be small (unless the policy gradient happens to be very large, which we find to be unlikely in practice). Intuitively, these are examples where the model either almost always gets the answer right (in which case there is nothing new to learn) or almost always gets it wrong (in which case the problem is currently too difficult to learn).</p>
<h3>Experimental Results</h3>
<p>We compare DASH to GRPO both in terms of accuracy and running time by training Qwen2.5-0.5B using both GRPO and DASH. We also use an ablation of DASH without gradient filtering, denoted No-GF. Results are shown in the tables below. As can be seen, DASH significantly reduces GRPO training time (from 38.9 hours to 6.6 hours) without any significant reduction in performance, highlighting the effectiveness of preemptive sampling and gradient filtering.
    <d-figure>
            <img src="images/dashvsgrpo.png" width="100%">
    </d-figure>
Compared to No-GF, DASH reduces running time by 4% without any significant reduction in performance. The effectiveness of gradient filtering can be improved to 10%; see the appendix of our paper. The impact of GF on training dynamics is illustrated in Figure 3. Specifically, as shown in Figure 3(a), gradient filtering
increases the average absolute advantage values, leading to more significant gradient updates; consequently, as shown in
Figure 3(b), forward and backward pass running times are reduced. Finally, since only samples inducing trivial gradient
updates are filtered out, the training curves remain similar before and after applying gradient filtering, as shown in Figure 3(c).</p>
        <d-figure>
            <img src="images/gf.png" width="100%">
        </d-figure>
    
    </section>


    <section id="pgvsppo">
        <h2>Policy Gradient vs. PPO Gradient Updates</h2>
        <h3> PPO has significantly higher variance compared to Policy Gradient, which is
the opposite of conventional wisdom </h3>
Note that since DASH takes one gradient step only for each new sample, it coincides with the Policy Gradient algorithm. On the other hand, PPO allows multiple gradient descents on each sample (for detailed comparison, check section 3.2 of our paper). We thus consider two implementations of PPO.
First, we can take K gradient steps using all M
samples, which we call PPO-Multi (or just Multi).
Second, we can divide the M examples into K
mini-batches of size M/K each, and take one gradient step on each mini-batch, which we call PPO-Mini (or just Mini).

<br> <br> 
We compare DASH to Multi and Mini. DASH uses a batch size of M = 256 (with K = 1), Multi uses M = 256
and K = 3, and Mini uses M = 8 so K = 32. Multi and Mini are slower than DASH; for fair comparison, we truncate
their training times to match the wall-clock time of DASH. The results are shown in Table 5, and training curves are shown
in Figure 4. As can be seen, Multi and Mini achieve faster initial performance improvements and have slightly higher
accuracies; however, they have significantly more unstable training curves. Similar results for the 1.5B model are in the appendix of our paper.
        <d-figure>
            <img src="images/multimini.png" width="100%">
        </d-figure>
 
    </section>




    <section id="kl">
        <h2>KL Divergence Regularization</h2>
        <h3> Removing KL divergence regularization can lead to more concise generations
and higher accuracies </h3>
 We compare DASH to an ablation without the KL divergence term, denoted No-KL. Training reward curves are shown
in Figure 6(a). As can be seen, removing KL divergence regularization generally leads to higher rewards during training;
most likely, No-KL can focus on reward optimization without being constrained to stay close to the initial model. As shown
in Table 6, No-KL achieves greater in- and out-of-distribution than DASH (except in the case of the out-of-distribution
accuracy of the 3B model).
<br> <br> 
Furthermore, as shown in Figure 6(b), we find that for No-KL, the average generation length is shorter, thereby reducing
overall training time; this difference is also reflected in Table 6. We hypothesize that to compensate for KL divergence
regularization, models must generate longer reasoning traces.
        <d-figure>
            <img src="images/nokl.png" width="100%">
        </d-figure>
        <d-figure>
            <center>
            <img src="images/nokltable.png" width="70%">
            </center>
        </d-figure>
                <h3> RL concentrates probability mass and reduces generation diversity </h3>

Finally, for the 3B model, we study how KL divergence regularization affects pass@k. Results are in Figure 5: No-KL performs best for small k, although the gap closes for
larger k. Intuitively, RL concentrates probability mass and reduces generation diversity (Shypula et al., 2025; West & Potts, 2025).
        <d-figure>
            <center>
            <img src="images/pass.png" width="60%">
            </center>
        </d-figure>

    </section>

   <section id="conclusion">
        <h2>Conclusion</h2>
 We have performed a careful empirical analysis of several key design decisions in RL algorithms for improving language model reasoning, with a focus on computationally constrained scenarios; these include SFT vs. on-policy RL, policy gradient vs. PPO, and whether KL divergence regularization is used. Furthermore, we identify the sampling strategy as the primary computational bottleneck in on-policy RL; to address these issues, we propose DASH, a novel algorithm using preemptive sampling and gradient filtering to improve efficiency. We demonstrate that DASH can reduce RL training time by 83\% while maintaining performance. More broadly, we believe that systematizing the study of RL for language model reasoning is key to designing more effective RL algorithms in this domain, which differs significantly from robotics domains targeted by existing RL algorithms such as PPO. Our study is a first step in this direction.

    </section>



        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{huang2025effectivereinforcementlearningreasoning,<br>
                &nbsp;&nbsp;title={Effective Reinforcement Learning for Reasoning in Language Models}, <br>
                &nbsp;&nbsp;author={Lianghuan Huang and Shuo Li and Sagnik Anupam and Insup Lee and Osbert Bastani},<br>
                &nbsp;&nbsp;year={2025},<br>
                &nbsp;&nbsp;eprint={2505.17218},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.AI},<br>
                &nbsp;&nbsp;url={https://arxiv.org/abs/2505.17218}, <br>
                }
            </p>
            <h3>Acknowledgment</h3>
            <p> Website template is borrowed from  <a href=" https://tsb0601.github.io/mmvp_blog/#mmvp" class="code-link">here</a>.</p>
        </d-appendix>


        <!-- <script type="text/bibliography">

        </script> -->
        <script>

            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box

            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }

            // Add the resize event listener
            window.addEventListener('resize', onResize);

            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }


            });



            // Initialize width and position
            onResize();
        </script>

        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;

                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }

            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');

                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }

            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);

            // Initial update
            updateNavigation();

            // Insert citations and update bibliography on page load
        </script>
    </body>
</html>
